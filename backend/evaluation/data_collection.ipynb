{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîΩ Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq openai bs4 tqdm pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json \n",
    "import pandas as pd \n",
    "import time \n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import os\n",
    "from groq import Groq\n",
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§≤üèª Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_groq_completion(prompt):\n",
    "    \"\"\"generates the completion for groq\n",
    "\n",
    "    Args:\n",
    "        prompt (str): the prompt for groq completion\n",
    "    \"\"\"\n",
    "    groq_API_key = os.environ(\"GROQ_API_KEY\")\n",
    "\n",
    "    client = Groq(\n",
    "        api_key=groq_API_key,\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶† Bioarxiv generate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_biorxiv_data(server, interval, cursor=0, format=\"json\"):\n",
    "    \"\"\"\n",
    "    Fetch data from bioRxiv API.\n",
    "\n",
    "    Args:\n",
    "        server (str): 'biorxiv' or 'medrxiv'.\n",
    "        interval (str): Date interval in 'YYYY-MM-DD/YYYY-MM-DD' or number of recent days/articles 'Nd' or 'N'.\n",
    "        cursor (int): Cursor for pagination.\n",
    "        format (str): Data format 'json' or 'xml'.\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response from the API.\n",
    "    \"\"\"\n",
    "    base_url = f\"https://api.biorxiv.org/details/{server}/{interval}/{cursor}/{format}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Error fetching data: {response.status_code}\")\n",
    "\n",
    "\n",
    "def extract_data(json_data):\n",
    "    \"\"\"\n",
    "    Extract relevant data from the API response.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): JSON response from the bioRxiv API.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries with extracted paper details.\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    for item in json_data[\"collection\"]:\n",
    "        paper_details = {\n",
    "            \"doi\": item[\"doi\"],\n",
    "            \"title\": item[\"title\"],\n",
    "            \"authors\": item[\"authors\"],\n",
    "            \"date\": item[\"date\"],\n",
    "            \"abstract\": item[\"abstract\"],\n",
    "        }\n",
    "        papers.append(paper_details)\n",
    "    return papers\n",
    "\n",
    "\n",
    "def get_biorxiv_data(server, start_date, end_date, max_results=100):\n",
    "    \"\"\"\n",
    "    Get data from bioRxiv within a date range.\n",
    "\n",
    "    Args:\n",
    "        server (str): 'biorxiv' or 'medrxiv'.\n",
    "        start_date (str): Start date in 'YYYY-MM-DD'.\n",
    "        end_date (str): End date in 'YYYY-MM-DD'.\n",
    "        max_results (int): Maximum number of results to fetch.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with the fetched data.\n",
    "    \"\"\"\n",
    "    interval = f\"{start_date}/{end_date}\"\n",
    "    cursor = 0\n",
    "    all_papers = []\n",
    "\n",
    "    while len(all_papers) < max_results:\n",
    "        data = fetch_biorxiv_data(server, interval, cursor)\n",
    "        papers = extract_data(data)\n",
    "        all_papers.extend(papers)\n",
    "        cursor += 100\n",
    "        if len(papers) < 100:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(all_papers)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "server = \"biorxiv\"\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2023-01-31\"\n",
    "max_results = 200\n",
    "\n",
    "df = get_biorxiv_data(server, start_date, end_date, max_results)\n",
    "# %%\n",
    "df.to_csv(\"biorxiv_data_abstract.csv\", index=False)\n",
    "\n",
    "\n",
    "####################### biorxiv_data QA generation #######################\n",
    "\n",
    "df = pd.read_csv(\"./biorxiv_data_abstract.csv\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def generate_questions(df, client):\n",
    "    responses = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            While researching for the latest research papers from biorxiv we came across the following research paper: {row['title']}, \n",
    "            Abstract of the research paper is {row['abstract']}\n",
    "            Can you generate 5 questions and answers for the following research paper that researcher might ask?\n",
    "\n",
    "            ---\n",
    "\n",
    "            - Answers should be list of dictionaries (JSON mode). \n",
    "            - Answers should be as mentioned in research paper don't add additional knowledge. \n",
    "            - Questions should have high intellectual and information value don't ask childish questoins like what's the name of research paper and stuff. \n",
    "            - Answers should be only from available information in abstract of the paper provided.\n",
    "\n",
    "            ---\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "            answer = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"you are expert researcher in healthcare domain.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            responses.append(json.loads(answer.choices[0].message.content))\n",
    "        except Exception as e:\n",
    "            time.sleep(30)\n",
    "            print(f\"Error in generating questions for index {i}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses\n",
    "\n",
    "\n",
    "answer = generate_questions(df.head(50), client)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "df[\"QAs\"] = None\n",
    "for idx, ans in enumerate(answer):\n",
    "    question_pattern = \"'question'\\s*:\\s*'([^']+)'\"\n",
    "    questions = re.findall(question_pattern, str(ans))\n",
    "    # questions = [q[0] for q in questions if q[0] != \"\"]\n",
    "    if len(questions) == 0:\n",
    "        question_pattern = \"'question'\\s*:\\s*\\\"([^\\\"]+)\"\n",
    "        questions = re.findall(question_pattern, str(ans))\n",
    "    answer_pattern = \"'answer':\\s*'([^']+)'\"\n",
    "    answers = re.findall(answer_pattern, str(ans))\n",
    "    try:\n",
    "        assert len(questions) == len(answers)\n",
    "        QAs = []\n",
    "\n",
    "        for i in range(len(questions)):\n",
    "            QAs.append({\"Question\": questions[i], \"Answer\": answers[i]})\n",
    "        df.at[idx, \"QAs\"] = str(QAs)\n",
    "    except AssertionError:\n",
    "        print(f\"Questions and Answers are not equal for index {idx}\")\n",
    "        print(f\"Questions: {questions}\")\n",
    "        print(f\"Answers: {answers}\")\n",
    "        df.at[idx, \"QAs\"] = None\n",
    "\n",
    "# %%\n",
    "df.to_csv(\"bioarxiv_abstracts.csv\", index=False)\n",
    "# %%\n",
    "########### making DF to store into the gsheet ############\n",
    "col_names = [\"Study Title\", \"Question\", \"Answer\", \"link\", \"source\"]\n",
    "store_df = pd.DataFrame(columns=col_names)\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"QAs\"] is not None:\n",
    "        for j in eval(row[\"QAs\"]):\n",
    "            store_df = pd.concat(\n",
    "                [\n",
    "                    store_df,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"Study Title\": [row[\"title\"]],\n",
    "                            \"Question\": [j[\"Question\"]],\n",
    "                            \"Answer\": [j[\"Answer\"]],\n",
    "                            \"link\": [row[\"doi\"]],\n",
    "                            \"source\": [\"bioarxiv\"],\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            # store_df = store_df.append({'Study Title': row['Title'], 'Question': j['Question'], 'Answer': j['Answer'], 'link': row['link'], 'source': row['disease']+\"pubmed\"}, ignore_index=True)\n",
    "# %%\n",
    "store_df.reset_index(drop=True).to_csv(\"bioarxiv_append_finale.csv\", index=False)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Chembl data extraction and generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_chembl(query, retries=5, timeout=10):\n",
    "    \"\"\"\n",
    "    Search ChEMBL for compounds related to a query\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query for ChEMBL data\n",
    "        retries (int, optional): number of retries in case of failure. Defaults to 5.\n",
    "        timeout (int, optional): request time out in case server doesn't respond. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        response_json: returns the response in json format.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.ebi.ac.uk/chembl/api/data/molecule\"\n",
    "    params = {\"search\": query, \"format\": \"json\"}\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} of {retries}: {e}\")\n",
    "            time.sleep(int(2.5**attempt))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_chembl_data(response_json, drug_name):\n",
    "    \"\"\"\n",
    "    Parse the JSON response from ChEMBL\n",
    "\n",
    "    Args:\n",
    "        response_json (dict): JSON response from ChEMBL API\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: returns a list of compounds with relevant data\n",
    "    \"\"\"\n",
    "    compounds = []\n",
    "\n",
    "    for item in response_json[\"molecules\"][:5]:\n",
    "        molecule_data = {\n",
    "            \"drug\": drug_name,\n",
    "            \"molecule_chembl_id\": item.get(\"molecule_chembl_id\"),\n",
    "            \"Molecule Properties\": {\n",
    "                \"alogp\": item.get(\"molecule_properties\", {}).get(\"alogp\"),\n",
    "                \"aromatic_rings\": item.get(\"molecule_properties\", {}).get(\n",
    "                    \"aromatic_rings\"\n",
    "                ),\n",
    "            },\n",
    "            \"Molecule ID and Structure\": {\n",
    "                \"canonical_smiles\": item.get(\"molecule_structures\", {}).get(\n",
    "                    \"canonical_smiles\"\n",
    "                ),\n",
    "            },\n",
    "            \"Chemical and Physical Nature\": {\n",
    "                \"molecule_type\": item.get(\"molecule_type\", \"\"),\n",
    "                \"chirality\": item.get(\"chirality\", \"\"),\n",
    "            },\n",
    "            \"Development and Approval Status\": {\n",
    "                \"first_approval\": item.get(\"first_approval\", \"\"),\n",
    "                \"black_box_warning\": item.get(\"black_box_warning\", \"\"),\n",
    "            },\n",
    "            \"Other Relevant Information\": {\n",
    "                \"oral\": item.get(\"oral\", \"\"),\n",
    "                \"therapeutic_flag\": item.get(\"therapeutic_flag\", \"\"),\n",
    "            },\n",
    "        }\n",
    "        compounds.append(molecule_data)\n",
    "\n",
    "    return compounds\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "def get_chembl_document():\n",
    "    \"\"\"\n",
    "    getting all the docuemnts\n",
    "\n",
    "    Returns:\n",
    "        json_response: response releated to the document\n",
    "    \"\"\"\n",
    "    url = \"https://www.ebi.ac.uk/chembl/api/data/document/\"\n",
    "\n",
    "    headers = {\"Accept\": \"application/json\"}  # Specify that you want JSON\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.headers.get(\"Content-Type\") == \"application/json\":\n",
    "        return response.json()  # Return the JSON response\n",
    "    else:\n",
    "        return \"Response was not in JSON format.\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "drugs = [\n",
    "    \"Aspirin\",\n",
    "    \"Paracetamol (Acetaminophen)\",\n",
    "    \"Ibuprofen\",\n",
    "    \"Metformin\",\n",
    "    \"Atorvastatin\",\n",
    "    \"Simvastatin\",\n",
    "    \"Lisinopril\",\n",
    "    \"Amlodipine\",\n",
    "    \"Amoxicillin\",\n",
    "    \"Ciprofloxacin\",\n",
    "    \"Doxycycline\",\n",
    "    \"Azithromycin\",\n",
    "    \"Prednisone\",\n",
    "    \"Warfarin\",\n",
    "    \"Insulin Glargine\",\n",
    "    \"Losartan\",\n",
    "    \"Omeprazole\",\n",
    "    \"Fluoxetine (Prozac)\",\n",
    "    \"Sertraline\",\n",
    "    \"Alprazolam\",\n",
    "]\n",
    "drug_data_df = pd.DataFrame()\n",
    "for query in drugs:\n",
    "    response_json = search_chembl(query)\n",
    "    if response_json:\n",
    "        compound_data = parse_chembl_data(response_json, drug_name=query)\n",
    "        df = pd.DataFrame(compound_data)\n",
    "        drug_data_df = pd.concat([drug_data_df, df])\n",
    "# %%\n",
    "\n",
    "drug_data_df = drug_data_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def generate_questions(df, client):\n",
    "    responses = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            While researching for about the {row['drug']} we came across the following information from chembl\n",
    "            {str(row.to_dict())}\n",
    "            Can you generate 5 questions and answers for the following research paper that researcher might ask?\n",
    "\n",
    "            ---\n",
    "\n",
    "            - Answers should be list of. \n",
    "            - Answers should be as mentioned in research paper don't add additional knowledge. \n",
    "            - Questions should have high intellectual and information value don't ask childish questoins like what's the name of research paper and stuff. \n",
    "            - Answers should be only from available information in abstract of the paper provided.\n",
    "\n",
    "            ---\n",
    "\n",
    "            example: \n",
    "            Q: What is the molecule_chembl_id for the {row['drug']}?\n",
    "            A: {row['molecule_chembl_id']}\n",
    "            ---\n",
    "            \"\"\"\n",
    "\n",
    "            answer = get_groq_completion(prompt)\n",
    "            responses.append(answer)\n",
    "            if i % 20 == 0:\n",
    "                time.sleep(60)\n",
    "        except Exception as e:\n",
    "            time.sleep(30)\n",
    "            print(f\"Error in generating questions for index {i}: {e}\")\n",
    "            responses.append(None)\n",
    "        #     responses.append(json.loads(answer.choices[0].message.content))\n",
    "        # except Exception as e:\n",
    "        #     time.sleep(30)\n",
    "        #     print(f\"Error in generating questions for index {i}: {e}\")\n",
    "        #     responses.append(None)\n",
    "    return responses\n",
    "\n",
    "\n",
    "# %%\n",
    "answer = generate_questions(drug_data_df.head(50), \"\")\n",
    "# %%\n",
    "\n",
    "drug_data_df[\"QAs\"] = None\n",
    "for idx, ans in enumerate(answer):\n",
    "    try:\n",
    "        questions = ans.split(\"Q:\")\n",
    "        QA_dict = {}\n",
    "        # print(questions)\n",
    "        for q in questions:\n",
    "            try:\n",
    "                question_data = q.split(\"A:\")[0].replace(\"\\n\", \"\").strip()\n",
    "                answer_data = q.split(\"A:\")[1].replace(\"\\n\", \"\").strip()\n",
    "                QA_dict[question_data] = answer_data\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    drug_data_df.at[idx, \"QAs\"] = str(QA_dict)\n",
    "    # df['QAs'] =\n",
    "# %%\n",
    "drug_data_df.to_csv(\"chembl_data.csv\", index=False)\n",
    "# %%\n",
    "col_names = [\"drug\", \"Question\", \"Answer\", \"chembl_id\", \"source\"]\n",
    "store_df = pd.DataFrame(columns=col_names)\n",
    "for i, row in drug_data_df.iterrows():\n",
    "    if row[\"QAs\"] is not None:\n",
    "        for k, v in eval(row[\"QAs\"]).items():\n",
    "            store_df = pd.concat(\n",
    "                [\n",
    "                    store_df,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"drug\": [row[\"drug\"]],\n",
    "                            \"Question\": [k],\n",
    "                            \"Answer\": [v],\n",
    "                            \"chembl_id\": [row[\"molecule_chembl_id\"]],\n",
    "                            \"source\": [\"chembl\"],\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            # store_df = store_df.append({'Study Title': row['Title'], 'Question': j['Question'], 'Answer': j['Answer'], 'link': row['link'], 'source': row['disease']+\"pubmed\"}, ignore_index=True)\n",
    "\n",
    "# %%\n",
    "store_df.to_csv(\"chembl_append_csv_finale.csv\", index=False)\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•º Pubmed data extraction and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_pubmed(query, retries=5, timeout=10):\n",
    "    \"\"\"\n",
    "    Search PubMed for papers related to a query\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query for pubmed data\n",
    "        retries (int, optional): number of retries in case of failure. Defaults to 5.\n",
    "        timeout (int, optional): request time out in case server doesn't respond to our given request. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        response_json: returns the response in json format the query and the json dictionary.\n",
    "    \"\"\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\"db\": \"pubmed\", \"term\": query, \"retmode\": \"json\", \"retmax\": 12}\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} of {retries}: {e}\")\n",
    "            time.sleep(int(2.5**attempt))\n",
    "\n",
    "    return None  # or handle this appropriately\n",
    "\n",
    "\n",
    "def get_abstracts_from_pubmed(paper_ids):\n",
    "    \"\"\"\n",
    "    Get abstracts for a list of paper IDs from PubMed\n",
    "\n",
    "    Args:\n",
    "        paper_ids (List[str]): List of paper IDs\n",
    "\n",
    "    Returns:\n",
    "        xml_abstract: returns the abstract in xml format\n",
    "    \"\"\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    abstracts = []\n",
    "    for paper_id in tqdm(paper_ids):\n",
    "        params = {\"db\": \"pubmed\", \"id\": paper_id, \"retmode\": \"xml\"}\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                # Extracting abstract from XML can be complex and depends on the XML structure\n",
    "                # Here we just return the raw XML for simplicity\n",
    "                abstracts.append(response.text)\n",
    "            else:\n",
    "                abstracts.append(\"Error fetching abstract\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching abstract: {e}\")\n",
    "            abstracts.append(\"Error fetching abstract\")\n",
    "\n",
    "    return abstracts\n",
    "\n",
    "\n",
    "def parse_pubmed_xml(xml_data):\n",
    "    \"\"\"\n",
    "    Parse XML data from PubMed\n",
    "\n",
    "    Args:\n",
    "        xml_data (str): XML data from PubMed\n",
    "\n",
    "    Returns:\n",
    "        parsed_data: returns the parsed data in json format\n",
    "    \"\"\"\n",
    "    root = ET.fromstring(xml_data)\n",
    "    articles = []\n",
    "\n",
    "    for article in root.findall(\".//PubmedArticle\"):\n",
    "        try:\n",
    "            article_data = {\n",
    "                \"PMID\": article.find(\".//PMID\").text,\n",
    "                \"Title\": article.find(\".//ArticleTitle\").text,\n",
    "                \"Abstract\": (\n",
    "                    article.find(\".//AbstractText\").text\n",
    "                    if article.find(\".//AbstractText\") is not None\n",
    "                    else \"No abstract\"\n",
    "                ),\n",
    "                \"Authors\": [\n",
    "                    auth.find(\"ForeName\").text + \" \" + auth.find(\"LastName\").text\n",
    "                    for auth in article.findall(\".//Author\")\n",
    "                ],\n",
    "            }\n",
    "            articles.append(article_data)\n",
    "        except Exception as e:\n",
    "            print(\"cannot parse article:\", e)\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Search for papers related to brain damage\n",
    "def get_json_data(search_diseases: list = []):\n",
    "    \"\"\"\n",
    "    Get json data from PubMed\n",
    "\n",
    "    Args:\n",
    "        search_diseases (List[str], optional): what diseases to serach on pubmed for abstracts. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        parsed_data: returns the parsed pubmed data with abstract in list of dictionaries.\n",
    "    \"\"\"\n",
    "    if len(search_diseases) == 0:\n",
    "        search_diseases = [\n",
    "            \"brain damage\",\n",
    "            \"bioinformatics\",\n",
    "            \"cancer\",\n",
    "            \"diabetes\",\n",
    "            \"brain hemorrhage\",\n",
    "        ]\n",
    "    final_data = []\n",
    "    for search_query in tqdm(search_diseases):\n",
    "        search_result = search_pubmed(search_query)\n",
    "\n",
    "        json_results = []\n",
    "        if search_result and \"esearchresult\" in search_result:\n",
    "            paper_ids = search_result[\"esearchresult\"][\"idlist\"]\n",
    "            abstracts = get_abstracts_from_pubmed(paper_ids)\n",
    "            for abstract in abstracts:\n",
    "                try:\n",
    "                    json_result = parse_pubmed_xml(abstract)\n",
    "                    json_result[0][\"disease\"] = search_query\n",
    "                    json_result[0][\n",
    "                        \"link\"\n",
    "                    ] = f\"https://pubmed.ncbi.nlm.nih.gov/{json_result[0]['PMID']}\"\n",
    "                    json_results.append(json_result[0])\n",
    "                except Exception as e:\n",
    "                    print(\"cannot parse abstract:\", e)\n",
    "            final_data.extend(json_results)\n",
    "        else:\n",
    "            print(\"Error in searching PubMed\")\n",
    "    return final_data\n",
    "\n",
    "\n",
    "################ PARSE AND SAVE TO DATAFRAME ################\n",
    "final_data = get_json_data()\n",
    "# %%\n",
    "df = pd.DataFrame(final_data)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "################ QUESTION ANSWERS GENERATION ################\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def generate_questions(df, client):\n",
    "    responses = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            While researching for the {row['disease']} we came across the following research paper: {row['Title']}, \n",
    "            Abstract of the research paper is {row['Abstract']}\n",
    "            Can you generate 5 questions and answers for the following research paper that researcher might ask?\n",
    "\n",
    "            ---\n",
    "\n",
    "            - Answers should be list of dictionaries (JSON mode). \n",
    "            - Answers should be as mentioned in research paper don't add additional knowledge. \n",
    "            - Questions should have high intellectual and information value don't ask childish questoins like what's the name of research paper and stuff. \n",
    "            - Answers should be only from available information in abstract of the paper provided.\n",
    "\n",
    "            ---\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "            answer = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"you are expert researcher in healthcare domain.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            responses.append(json.loads(answer.choices[0].message.content))\n",
    "        except Exception as e:\n",
    "            time.sleep(30)\n",
    "            print(f\"Error in generating questions for index {i}: {e}\")\n",
    "            responses.append(None)\n",
    "    return responses\n",
    "\n",
    "\n",
    "answer = generate_questions(df.head(50), client)\n",
    "\n",
    "# %%\n",
    "df[\"QAs\"] = None\n",
    "for idx, ans in enumerate(answer):\n",
    "    question_pattern = \"'question'\\s*:\\s*'([^']+)'\"\n",
    "    questions = re.findall(question_pattern, str(ans))\n",
    "    # questions = [q[0] for q in questions if q[0] != \"\"]\n",
    "    if len(questions) == 0:\n",
    "        question_pattern = \"'question'\\s*:\\s*\\\"([^\\\"]+)\"\n",
    "        questions = re.findall(question_pattern, str(ans))\n",
    "    answer_pattern = \"'answer':\\s*'([^']+)'\"\n",
    "    answers = re.findall(answer_pattern, str(ans))\n",
    "    try:\n",
    "        assert len(questions) == len(answers)\n",
    "        QAs = []\n",
    "\n",
    "        for i in range(len(questions)):\n",
    "            QAs.append({\"Question\": questions[i], \"Answer\": answers[i]})\n",
    "        df.at[idx, \"QAs\"] = str(QAs)\n",
    "    except AssertionError:\n",
    "        print(f\"Questions and Answers are not equal for index {idx}\")\n",
    "        print(f\"Questions: {questions}\")\n",
    "        print(f\"Answers: {answers}\")\n",
    "        df.at[idx, \"QAs\"] = None\n",
    "\n",
    "# %%\n",
    "df.to_csv(\"pubmed_abstracts.csv\", index=False)\n",
    "# %%\n",
    "########### making DF to store into the gsheet ############\n",
    "col_names = [\"Study Title\", \"Question\", \"Answer\", \"link\", \"source\"]\n",
    "store_df = pd.DataFrame(columns=col_names)\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"QAs\"] is not None:\n",
    "        for j in eval(row[\"QAs\"]):\n",
    "            store_df = pd.concat(\n",
    "                [\n",
    "                    store_df,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"Study Title\": [row[\"Title\"]],\n",
    "                            \"Question\": [j[\"Question\"]],\n",
    "                            \"Answer\": [j[\"Answer\"]],\n",
    "                            \"link\": [row[\"link\"]],\n",
    "                            \"source\": [row[\"disease\"] + \"_pubmed\"],\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            # store_df = store_df.append({'Study Title': row['Title'], 'Question': j['Question'], 'Answer': j['Answer'], 'link': row['link'], 'source': row['disease']+\"pubmed\"}, ignore_index=True)\n",
    "# %%\n",
    "store_df.reset_index(drop=True).to_csv(\"pubmed_append_csv_finale.csv\", index=False)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü©∫ Clinical trials data storage and generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conditions_trial_list = [\n",
    "    \"heart attack\",\n",
    "    \"Lung cancer\",\n",
    "    \"Diabetes\",\n",
    "    \"covid-19\",\n",
    "    \"high blood pressure\",\n",
    "    \"asthma\",\n",
    "]\n",
    "\n",
    "link = \"https://clinicaltrials.gov/api/rss?cond=heart+attack&dateField=StudyFirstPostDate\"\n",
    "response = requests.get(link)\n",
    "\n",
    "fin_df = pd.DataFrame(columns=[\"Study Title\", \"Question\", \"Answer\", \"link\"])\n",
    "# %%\n",
    "map_dict = {\n",
    "    \"Exposure, Dose, Body Burden and Health Effects of Lead\": \"https://clinicaltrials.gov/study/NCT00013819?cond=Lead%20Poisoning&limit=10&rank=5\",\n",
    "    \"Homeopathic Preparation Plumbum Metallicum for Lead Poisoning\": \"https://clinicaltrials.gov/study/NCT00931905?cond=Lead%20Poisoning&limit=10&rank=4\",\n",
    "    \"The Combined Effect of 2,3-Dimercaptosuccinic Acid and Multi-Nutrients on Children in Lead Poisoning\": \"https://clinicaltrials.gov/study/NCT00374894?cond=Lead%20Poisoning&limit=10&rank=6\",\n",
    "}\n",
    "\n",
    "for i, link in map_dict.items():\n",
    "    question_formats = [\n",
    "        f\"What is the purpose of the study {i}\",\n",
    "        f\"How many participants was enrolled in {i}\",\n",
    "        f\"What treatment was given to the participants in {i}\",\n",
    "        f\"Where did the study {i} take place\",\n",
    "        f\"What is the type of following study: {i}\",\n",
    "    ]\n",
    "\n",
    "    for q in question_formats:\n",
    "        fin_df = pd.concat(\n",
    "            [\n",
    "                fin_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Study Title\": [i],\n",
    "                        \"Question\": [q],\n",
    "                        \"Answer\": [None],\n",
    "                        \"link\": link,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "fin_df.to_csv(\"questions.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí∞ Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import dspy \n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import requests\n",
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "\n",
    "\n",
    "# CONFIG\n",
    "random_state = 42\n",
    "txt_to_idx = {\n",
    "    'Clicnical Trials': '0',\n",
    "    'pubmed': '1',\n",
    "    'bioarxiv': '2',\n",
    "    'chembl': '3'\n",
    "}\n",
    "\n",
    "NUM_THREADS = 5\n",
    "DEV_NUM = 50\n",
    "\n",
    "idx_to_txt = {v: k for k, v in txt_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dspy setting up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo', api_key = os.environ[\"OPENAPI_KEY\"])\n",
    "# rm module is currently not available.\n",
    "dspy.settings.configure(lm=turbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé∞ preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_source(x): \n",
    "    \"\"\"\n",
    "    Generates the routing label current source don't have proper labels we have appended disease_type + _ + pubmed for pubmed data\n",
    "    fixing the above to return standardized source. \n",
    "\n",
    "    Args:\n",
    "        x (str): standardized source string\n",
    "    \"\"\"\n",
    "    if x.split(\"_\")[-1] == \"pubmed\":\n",
    "        return \"pubmed\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df = pd.read_csv(\"../evaluation/question_answers_eval.csv\")\n",
    "df['route_option'] = df['Source'].apply(lambda x: get_router_source(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl_sample = df[df['route_option'] == 'chembl'].sample(n=49, random_state=random_state)\n",
    "pubmed_sample = df[df['route_option'] == 'pubmed'].sample(n=49, random_state=random_state)\n",
    "clinical_trials = df[df['route_option'] == 'Clicnical Trials'].sample(n=49, random_state=random_state)\n",
    "bioarxiv_sample = df[df['route_option'] == 'bioarxiv'].sample(n=49, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = pd.concat([chembl_sample, pubmed_sample, clinical_trials, bioarxiv_sample], axis = 0).sample(frac = 1)\n",
    "total_data = []\n",
    "for idx, row in enumerate(all_samples.iterrows()): \n",
    "    dspy_example = dspy.Example({'question': row[1][\"Question\"], 'answer' : txt_to_idx[row[1]['route_option']]}).with_inputs(\"question\")\n",
    "    total_data.append(dspy_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚®ç Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does the knockout of Prx4 from endothelial cells affect vulnerability to ischemia/reperfusion (I/R) injury in mice?\n",
      "Predicted Answer: 1\n"
     ]
    }
   ],
   "source": [
    "# setting up and testing the basic signature\n",
    "class QA(dspy.Signature):\n",
    "    question = dspy.InputField(desc = \"Question to be routed to route\")\n",
    "    answer = dspy.OutputField(desc=\"route option should be integer only no additional text\")\n",
    "\n",
    "# testing out QA \n",
    "generate_answer = dspy.Predict(QA)\n",
    "dev_example = total_data[0]\n",
    "# Call the predictor on a particular input.\n",
    "pred = generate_answer(question=dev_example.question )\n",
    "\n",
    "# Print the input and the prediction.\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_example.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: Question to be routed to route\n",
      "Answer: route option should be integer only no additional text\n",
      "\n",
      "---\n",
      "\n",
      "Question: How does the knockout of Prx4 from endothelial cells affect vulnerability to ischemia/reperfusion (I/R) injury in mice?\n",
      "Answer:\u001b[32m 1\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router_module(dspy.Module):\n",
    "    f\"\"\"\n",
    "    Routes the specific question to relavant service we have following services as option {txt_to_idx}\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(QA)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        prediction = self.generate_answer(question=question)\n",
    "        return dspy.Prediction(answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚äπ Metrics definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(gold, pred, trace = None ):\n",
    "    actual_answer , pred_answer = gold.answer , pred.answer\n",
    "    print('actual_answer', actual_answer) \n",
    "    print('predicted_answer', pred_answer)\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(gold,pred)\n",
    "    return answer_EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíâ BootstrapFewShot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [00:01<01:09,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [00:23<11:04, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 3/50 [00:45<13:35, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 4/50 [01:27<20:48, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 5/50 [01:49<19:00, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 6/50 [02:31<22:44, 31.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 7/50 [03:14<24:55, 34.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 8/50 [03:36<21:31, 30.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 9/50 [04:24<24:42, 36.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [05:06<25:23, 38.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 11/50 [05:28<19:24, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 0\n",
      "Bootstrapped 4 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "teleprompter = BootstrapFewShot(metric=metric)\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(Router_module(), trainset=total_data[:DEV_NUM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_answer = ChainOfThought(Signature(question -> answer\n",
       "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Question to be routed to route', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'route option should be integer only no additional text', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
       "))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: Question to be routed to route\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "Answer: route option should be integer only no additional text\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the purpose of the study Ilomedin Treatment for Patients Having Undergone Primary Percutaneous Coronary Intervention (PCI) ?\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the need for constructing a novel TMSD based electrochemical detection strategy according to the research paper?\n",
      "Answer: 1\n",
      "\n",
      "---\n",
      "\n",
      "Question: How do mineral composition, surface topography, and surface charge influence DNA adsorption behavior and preservation on mineral substrates?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: Who is not recommending this work to be cited as a reference for projects?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: Where did the study Safety and Tolerability of RNS60 Given by IV to Healthy Subjects take place\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: In what ways does DigitISA offer an improvement over conventional immuno-sensing techniques?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: How do the results of utilizing GPLA jointly with biophysical modeling contribute to understanding the spatio-temporal dynamics observed in Utah array recordings?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: What does the second wave of muscle activity, time-locked to voluntary reach onset, indicate?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: What additional information beyond concentration measurements can DigitISA provide?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: Where did the study ROsuvastatin Pretreatment to Reduce MyocArdial Periprocedural Necrosis:Comparison With Atorvastatin Reloading (ROMAIIReload) take place\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: What type of health outcomes were measured in relation to medication adherence among the persons with MS in this study?\n",
      "Answer: 1\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the type of following study: Recipient Vessels for Free Tissue Flaps in Advanced Oncologic Defects of the Midface and Scalp\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: What treatment was given to the participants in Recipient Vessels for Free Tissue Flaps in Advanced Oncologic Defects of the Midface and Scalp\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: What was the mean MARS-5 score among persons with MS in the study?\n",
      "Answer: 1\n",
      "\n",
      "---\n",
      "\n",
      "Question: What are the surgical treatments mentioned for hydrocephalus in the research paper?\n",
      "Answer: 1\n",
      "\n",
      "---\n",
      "\n",
      "Question: What was the reason for withdrawing the manuscript?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: Where did the study Homeopathic Preparation Plumbum Metallicum for Lead Poisoning take place\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We need to identify the location where the study was conducted.\n",
      "Answer: 0\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag.save(\"router_bootstrapped_12_eg.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™ß BayesianSignature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [00:01<01:11,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer The specific individual or organization should be named.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [00:02<01:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 3/50 [00:24<08:15, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 4/50 [00:45<11:25, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 5/50 [01:07<13:03, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 6/50 [01:29<13:46, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer Ilomedin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 7/50 [01:50<14:06, 19.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 8/50 [02:12<14:10, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 9/50 [02:55<18:41, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [03:16<17:06, 25.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 11/50 [03:38<15:49, 24.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer C9H8O4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 12/50 [03:59<14:53, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 13/50 [04:22<14:13, 23.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 14/50 [04:44<13:41, 22.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 15/50 [05:05<13:01, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer Germany\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [05:28<12:44, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [05:50<12:17, 22.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer route option should be integer only no additional text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [06:32<15:09, 28.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer C17H18FN3O3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [06:54<11:16, 21.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 3\n",
      "Bootstrapped 1 full traces after 20 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [00:22<18:03, 22.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [00:44<17:37, 22.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 3/50 [01:05<17:05, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 4/50 [01:06<12:48, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 1\n",
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m teleprompter_baysian \u001b[38;5;241m=\u001b[39m BayesianSignatureOptimizer(prompt_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoute the respective question to the required service\u001b[39m\u001b[38;5;124m\"\u001b[39m, task_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroute integer option\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric\u001b[38;5;241m=\u001b[39mmetric, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_threads\u001b[38;5;241m=\u001b[39mNUM_THREADS, display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, display_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m compiled_router_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mteleprompter_baysian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRouter_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdevset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mDEV_NUM\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptuna_trials_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bootstrapped_demos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_labeled_demos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/teleprompt/signature_opt_bayesian.py:301\u001b[0m, in \u001b[0;36mBayesianSignatureOptimizer.compile\u001b[0;34m(self, student, devset, optuna_trials_num, max_bootstrapped_demos, max_labeled_demos, eval_kwargs, seed, view_data, view_examples)\u001b[0m\n\u001b[1;32m    298\u001b[0m             demo_candidates[\u001b[38;5;28mid\u001b[39m(module_p)]\u001b[38;5;241m.\u001b[39mappend(candidate_p\u001b[38;5;241m.\u001b[39mdemos)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Generate N candidate prompts\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m instruction_candidates, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_first_N_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Initialize variables to store the best program and its score\u001b[39;00m\n\u001b[1;32m    304\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/teleprompt/signature_opt_bayesian.py:176\u001b[0m, in \u001b[0;36mBayesianSignatureOptimizer._generate_first_N_candidates\u001b[0;34m(self, module, N, view_data, view_examples, demo_candidates, devset)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_model):\n\u001b[0;32m--> 176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observe_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservations:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m view_examples:\n\u001b[1;32m    179\u001b[0m     example_sets \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/teleprompt/signature_opt_bayesian.py:131\u001b[0m, in \u001b[0;36mBayesianSignatureOptimizer._observe_data\u001b[0;34m(self, trainset)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_observe_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainset):\n\u001b[1;32m    130\u001b[0m     upper_lim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trainset), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mview_data_batch_size)\n\u001b[0;32m--> 131\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDatasetDescriptor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mupper_lim\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     observations \u001b[38;5;241m=\u001b[39m observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    134\u001b[0m     skips \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/predict/predict.py:49\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/predict/predict.py:90\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m template \u001b[38;5;241m=\u001b[39m signature_to_template(signature)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Note: query_only=True means the instructions and examples are not included.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# I'm not really sure why we'd want to do that, but it's there.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dsp/primitives/predict.py:77\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[0;32m---> 77\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# BayesianSignature optimizer is not working \n",
    "# teleprompter_baysian = BayesianSignatureOptimizer(prompt_model=\"Route the respective question to the required service\", task_model=\"route integer option\", metric=metric, n=3)\n",
    "# kwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)\n",
    "# compiled_router_prompt = teleprompter_baysian.compile(Router_module(),  devset=total_data[:DEV_NUM], optuna_trials_num=100, max_bootstrapped_demos=1, max_labeled_demos=2, eval_kwargs=kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dspy.teleprompt.signature_opt_typed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mteleprompt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignature_opt_typed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize_signature\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m answer_exact_match\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypedChainOfThought\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dspy.teleprompt.signature_opt_typed'"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt.signature_opt_typed import optimize_signature\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from dspy.functional import TypedChainOfThought\n",
    "\n",
    "compiled_program = optimize_signature(\n",
    "    student=TypedChainOfThought(\"question -> answer\"),\n",
    "    evaluator=Evaluate(devset=total_data[:DEV_NUM], metric=answer_exact_match, num_threads=10, display_progress=True),\n",
    "    n_iterations=50,\n",
    ").program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ•Ô∏è Bootstrap few shot with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 2 traces per predictor.\n",
      "Will attempt to train 8 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 7/50 [00:00<00:00, 334.19it/s]\n",
      "[I 2024-03-12 19:53:33,909] A new study created in memory with name: no-name-e0f37a3a-69ec-448e-bba4-73dbd777928f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n",
      "actual_answer 1\n",
      "predicted_answer 2\n",
      "actual_answer 2\n",
      "predicted_answer 2\n",
      "actual_answer 3\n",
      "predicted_answer 2\n",
      "actual_answer 1\n",
      "predicted_answer 2\n",
      "actual_answer 3\n",
      "predicted_answer 2\n",
      "actual_answer 2\n",
      "predicted_answer 2\n",
      "Bootstrapped 2 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [00:01<00:04,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 2  (50.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:01<00:01,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 3  (33.3):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:21<00:19,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 2\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 4  (50.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:42<00:13, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:03<00:00, 12.76s/it]\n",
      "/opt/homebrew/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:142: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n",
      "[I 2024-03-12 19:54:37,873] Trial 0 finished with value: 60.0 and parameters: {'demo_index_for_generate_answer': 6}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 1\n",
      "Average Metric: 3 / 5  (60.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [00:21<01:26, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 2  (50.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:42<01:03, 21.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 3  (33.3):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:02<00:41, 20.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 4  (25.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:23<00:20, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:44<00:00, 20.93s/it]\n",
      "[I 2024-03-12 19:56:23,242] Trial 1 finished with value: 20.0 and parameters: {'demo_index_for_generate_answer': 0}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Average Metric: 1 / 5  (20.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [00:41<02:47, 41.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 0\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 2  (0.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [01:23<02:04, 41.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer patient outcome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 3  (0.0):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:43<01:03, 31.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "actual_answer 2\n",
      "predicted_answer 0\n",
      "Backing off 2.4 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.2 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.4 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 5.4 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 15.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 3.0 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.1 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 38.6 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 51.7 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 124.6 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 70.7 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 220.6 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 180.4 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 40.9 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 119.2 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 4  (0.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [11:35<04:13, 253.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Backing off 230.1 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [17:08<00:00, 205.77s/it]\n",
      "[I 2024-03-12 20:13:32,172] Trial 2 finished with value: 0.0 and parameters: {'demo_index_for_generate_answer': 5}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Average Metric: 0.0 / 5  (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.7 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [01:46<07:04, 106.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.4 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "actual_answer 3\n",
      "predicted_answer 0\n",
      "Backing off 3.1 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.9 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.5 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.7 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.5 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 5.0 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 10.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 8.5 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.9 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 18.0 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 8.4 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 57.5 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.0 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 63.8 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 32.6 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.0 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 202.8 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 54.0 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 8.1 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 48.8 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 135.5 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 220.4 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 184.5 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 2  (0.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [08:55<14:48, 296.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n",
      "Backing off 136.6 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 232.3 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 355.2 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 269.7 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 121.6 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 3  (33.3):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [16:20<12:08, 364.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.0 / 4  (25.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [16:53<03:53, 233.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.0 / 5  (20.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [17:24<00:00, 208.99s/it]\n",
      "[I 2024-03-12 20:30:57,151] Trial 3 finished with value: 20.0 and parameters: {'demo_index_for_generate_answer': 12}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Average Metric: 1.0 / 5  (20.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.0 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.2 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.5 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.8 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.9 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.8 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.2 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.0 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.8 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 8.6 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 11.7 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 10.0 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 31.8 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 20.8 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 16.6 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.9 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 21.6 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 37.7 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.7 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.1 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.5 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 29.3 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 115.9 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 110.0 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.1 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 30.5 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 54.8 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 239.5 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 146.2 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 413.4 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 48.7 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 110.3 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 373.3 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 444.4 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 206.1 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 127.3 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [13:00<52:01, 780.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Backing off 177.8 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 126.3 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 97.4 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 3  (0.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [17:05<23:16, 465.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [17:05<02:53, 173.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [17:43<00:00, 212.63s/it]\n",
      "[I 2024-03-12 20:48:40,365] Trial 4 finished with value: 0.0 and parameters: {'demo_index_for_generate_answer': 3}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Average Metric: 0.0 / 5  (0.0%)\n",
      "actual_answer 2\n",
      "predicted_answer patient outcome\n",
      "actual_answer 3\n",
      "predicted_answer 0\n",
      "actual_answer 2\n",
      "predicted_answer 0\n",
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 4  (0.0):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:00<00:00, 812.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.9 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.5 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 15.8 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:34<00:00, 30.90s/it]\n",
      "[I 2024-03-12 20:51:14,876] Trial 5 finished with value: 20.0 and parameters: {'demo_index_for_generate_answer': 5}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 1\n",
      "Average Metric: 1 / 5  (20.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.2 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.0 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.4 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.8 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 5.4 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.9 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.2 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.9 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.7 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 29.9 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.5 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.6 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 12.7 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 21.5 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 19.2 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 48.1 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.6 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 29.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 63.9 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 127.5 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.4 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 33.9 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 238.9 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 112.8 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 47.3 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 40.4 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [07:23<29:33, 443.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Backing off 180.1 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 505.5 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 167.4 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 153.3 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 68.8 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 54.8 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 274.6 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 241.3 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 208.5 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 2  (0.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [16:50<25:48, 516.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 3  (0.0):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [16:59<09:29, 284.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [17:24<03:02, 182.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [17:42<00:00, 212.60s/it]\n",
      "[I 2024-03-12 21:08:57,897] Trial 6 finished with value: 0.0 and parameters: {'demo_index_for_generate_answer': 10}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Average Metric: 0.0 / 5  (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.5 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.6 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.5 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.4 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.9 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.2 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.4 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.0 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 12.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 28.3 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 29.8 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 27.7 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 31.2 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 56.4 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.8 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 21.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 51.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 55.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 103.8 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 17.1 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 13.8 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 109.9 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 28.5 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 104.5 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 117.8 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 203.6 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 58.3 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 84.0 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 142.9 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 328.4 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 102.3 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 274.5 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 91.4 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 352.5 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 2.4 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 3.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 259.5 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 11.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 29.6 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 147.9 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 23.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 71.7 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 42.3 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 184.3 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 37.1 seconds after 12 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 2  (0.0):  20%|‚ñà‚ñà        | 1/5 [17:23<1:09:35, 1043.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [17:43<03:01, 181.70s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [19:29<00:00, 233.92s/it]\n",
      "[I 2024-03-12 21:28:27,540] Trial 7 finished with value: 0.0 and parameters: {'demo_index_for_generate_answer': 1}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Average Metric: 0.0 / 5  (0.0%)\n",
      "Best score: 60.0\n",
      "Best program: generate_answer = ChainOfThought(Signature(question -> answer\n",
      "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Question to be routed to route', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'route option should be integer only no additional text', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithOptuna\n",
    "\n",
    "fewshot_optuna_optimizer = BootstrapFewShotWithOptuna(metric=metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\n",
    "\n",
    "your_dspy_program_compiled = fewshot_optuna_optimizer.compile(student=Router_module(),max_demos = 2,  trainset=total_data[:DEV_NUM], valset=total_data[DEV_NUM: DEV_NUM + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_dspy_program_compiled.save(\"router_bootstrapped_few_shot_optuna.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: Question to be routed to route\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "Answer: route option should be integer only no additional text\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the role of CRISPR-Cas systems in Enterococcus faecalis strains?\n",
      "Reasoning: Let's think step by step in order to Answer: 2\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: How many aromatic rings are present in the structure of Lisinopril?\n",
      "Reasoning: Let's think step by step in order to Answer: 1\n",
      "Answer:\u001b[32m 1\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

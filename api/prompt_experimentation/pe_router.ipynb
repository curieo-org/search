{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí∞ Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/som/Downloads/code/search/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import dspy \n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import requests\n",
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "\n",
    "\n",
    "# CONFIG\n",
    "random_state = 42\n",
    "txt_to_idx = {\n",
    "    'Clicnical Trials': '0',\n",
    "    'pubmed': '1',\n",
    "    'bioarxiv': '2',\n",
    "    'chembl': '3'\n",
    "}\n",
    "\n",
    "NUM_THREADS = 5\n",
    "DEV_NUM = 50\n",
    "\n",
    "idx_to_txt = {v: k for k, v in txt_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dspy setting up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo', api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "mixtral = dspy.GROQ( api_key = os.environ['GROQ_API_KEY'], model = \"mixtral-8x7b-32768\",)\n",
    "# rm module is currently not available.\n",
    "\n",
    "dspy.settings.configure(lm=mixtral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé∞ preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_source(x): \n",
    "    \"\"\"\n",
    "    Generates the routing label current source don't have proper labels we have appended disease_type + _ + pubmed for pubmed data\n",
    "    fixing the above to return standardized source. \n",
    "\n",
    "    Args:\n",
    "        x (str): standardized source string\n",
    "    \"\"\"\n",
    "    if x.split(\"_\")[-1] == \"pubmed\":\n",
    "        return \"pubmed\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df = pd.read_csv(\"../evaluation/question_answers_eval.csv\")\n",
    "df['route_option'] = df['Source'].apply(lambda x: get_router_source(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl_sample = df[df['route_option'] == 'chembl'].sample(n=49, random_state=random_state)\n",
    "pubmed_sample = df[df['route_option'] == 'pubmed'].sample(n=49, random_state=random_state)\n",
    "clinical_trials = df[df['route_option'] == 'Clicnical Trials'].sample(n=49, random_state=random_state)\n",
    "bioarxiv_sample = df[df['route_option'] == 'bioarxiv'].sample(n=49, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = pd.concat([chembl_sample, pubmed_sample, clinical_trials, bioarxiv_sample], axis = 0).sample(frac = 1)\n",
    "total_data = []\n",
    "for idx, row in enumerate(all_samples.iterrows()): \n",
    "    dspy_example = dspy.Example({'question': row[1][\"Question\"], 'answer' : txt_to_idx[row[1]['route_option']]}).with_inputs(\"question\")\n",
    "    total_data.append(dspy_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚®ç Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the molecular formula for Aspirin?\n",
      "Predicted Answer: The molecular formula for Aspirin is C9H8O4. This chemical compound, also known as acetylsalicylic acid, has a molecular weight of 180.15 g/mol and is commonly used as a pain reliever, fever reducer, and anti-inflammatory drug.\n"
     ]
    }
   ],
   "source": [
    "# setting up and testing the basic signature\n",
    "class QA(dspy.Signature):\n",
    "    \"Study the `question` thoroughly to understand the context and meaning of the query before generating the `answer` based on accurate information and relevancy.\"\n",
    "    question = dspy.InputField(desc = \"Question to be routed to route\")\n",
    "    answer = dspy.OutputField(desc=\"route option should be integer only no additional text\", prefix = \"The answer to the question is:\")\n",
    "\n",
    "# testing out QA \n",
    "generate_answer = dspy.Predict(QA, n = 1 )\n",
    "dev_example = total_data[0]\n",
    "# Call the predictor on a particular input.\n",
    "pred = generate_answer(question=dev_example.question )\n",
    "\n",
    "# Print the input and the prediction.\n",
    "print(f\"Question: {dev_example.question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_example.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router_module(dspy.Module):\n",
    "    f\"\"\"\n",
    "    Routes the specific question to relavant service we have following services as option {txt_to_idx}\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(QA)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        prediction = self.generate_answer(question=question)\n",
    "        return dspy.Prediction(answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™ß Signature optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[WARNING] SignatureOptimizer has been deprecated and replaced with COPRO.  SignatureOptimizer will be removed in a future release. \u001b[31m\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x12c53df80> with kwargs {'n': 9, 'temperature': 1.4}\n",
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x12c53df80> with kwargs {'n': 9, 'temperature': 1.4}\n",
      "Backing off 0.8 seconds after 3 tries calling function <function GPT3.request at 0x12c53df80> with kwargs {'n': 9, 'temperature': 1.4}\n",
      "Backing off 5.0 seconds after 4 tries calling function <function GPT3.request at 0x12c53df80> with kwargs {'n': 9, 'temperature': 1.4}\n",
      "Backing off 11.9 seconds after 5 tries calling function <function GPT3.request at 0x12c53df80> with kwargs {'n': 9, 'temperature': 1.4}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/openai/_base_client.py:959\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 959\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     14\u001b[0m teleprompter \u001b[38;5;241m=\u001b[39m SignatureOptimizer(\n\u001b[1;32m     15\u001b[0m     metric\u001b[38;5;241m=\u001b[39mvalidate_context_and_answer,\n\u001b[1;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, display_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Used in Evaluate class in the optimization process\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m compiled_prompt_opt \u001b[38;5;241m=\u001b[39m \u001b[43mteleprompter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRouter_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mDEV_NUM\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dspy/teleprompt/signature_opt.py:41\u001b[0m, in \u001b[0;36mSignatureOptimizer.compile\u001b[0;34m(self, student, devset, eval_kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, student, \u001b[38;5;241m*\u001b[39m, devset, eval_kwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dspy/teleprompt/copro_optimizer.py:138\u001b[0m, in \u001b[0;36mCOPRO.compile\u001b[0;34m(self, student, trainset, eval_kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         instruct \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(BasicGenerateInstruction, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreadth\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_temperature)(basic_instruction\u001b[38;5;241m=\u001b[39mbasic_instruction)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     instruct \u001b[38;5;241m=\u001b[39m \u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBasicGenerateInstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbreadth\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_temperature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_instruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasic_instruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Add in our initial prompt as a candidate as well\u001b[39;00m\n\u001b[1;32m    140\u001b[0m instruct\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mproposed_instruction\u001b[38;5;241m.\u001b[39mappend(basic_instruction)\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:49\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:91\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m template \u001b[38;5;241m=\u001b[39m signature_to_template(signature)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Note: query_only=True means the instructions and examples are not included.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# I'm not really sure why we'd want to do that, but it's there.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dsp/primitives/predict.py:77\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[0;32m---> 77\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dsp/modules/gpt3.py:184\u001b[0m, in \u001b[0;36mGPT3.__call__\u001b[0;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m return_sorted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# if kwargs.get(\"n\", 1) > 1:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m#     if self.model_type == \"chat\":\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m#         kwargs = {**kwargs}\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#         kwargs = {**kwargs, \"logprobs\": 5}\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mlog_openai_usage:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_usage(response)\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dsp/modules/gpt3.py:150\u001b[0m, in \u001b[0;36mGPT3.request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dsp/modules/gpt3.py:123\u001b[0m, in \u001b[0;36mGPT3.basic_request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[1;32m    122\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(kwargs)}\n\u001b[0;32m--> 123\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dsp/modules/gpt3.py:271\u001b[0m, in \u001b[0;36mchat_request\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m OPENAI_LEGACY:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cached_gpt3_turbo_request_v2_wrapped(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv1_cached_gpt3_turbo_request_v2_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dsp/modules/cache_utils.py:16\u001b[0m, in \u001b[0;36mnoop_decorator.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dsp/modules/gpt3.py:264\u001b[0m, in \u001b[0;36mv1_cached_gpt3_turbo_request_v2_wrapped\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m cache_turn_on \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;129m@NotebookCacheMemory\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mv1_cached_gpt3_turbo_request_v2_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv1_cached_gpt3_turbo_request_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/joblib/memory.py:655\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/joblib/memory.py:598\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    595\u001b[0m     must_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_call:\n\u001b[0;32m--> 598\u001b[0m     out, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;66;03m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;66;03m# later calls\u001b[39;00m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/joblib/memory.py:856\u001b[0m, in \u001b[0;36mMemorizedFunc.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28mprint\u001b[39m(format_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, args, kwargs))\n\u001b[0;32m--> 856\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_backend\u001b[38;5;241m.\u001b[39mdump_item(\n\u001b[1;32m    858\u001b[0m     [func_id, args_id], output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[1;32m    860\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/dsp/modules/gpt3.py:258\u001b[0m, in \u001b[0;36mv1_cached_gpt3_turbo_request_v2\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    257\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Downloads/code/search/.venv/lib/python3.11/site-packages/openai/_base_client.py:1011\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1007\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1014\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1015\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1019\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# signature optimizer \n",
    "from dspy.teleprompt import SignatureOptimizer\n",
    "from dspy.evaluate import Evaluate\n",
    "dspy.settings.configure(lm=turbo)\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    return answer_EM\n",
    "\n",
    "NUM_THREADS = 5\n",
    "evaluate = Evaluate(devset=total_data[:DEV_NUM], metric=validate_context_and_answer, num_threads=NUM_THREADS, display_progress=True, display_table=False)\n",
    "\n",
    "\n",
    "\n",
    "teleprompter = SignatureOptimizer(\n",
    "    metric=validate_context_and_answer,\n",
    "    verbose=True,\n",
    ")\n",
    "kwargs = dict(num_threads=64, display_progress=True, display_table=0) # Used in Evaluate class in the optimization process\n",
    "\n",
    "compiled_prompt_opt = teleprompter.compile(Router_module(), devset=total_data[:DEV_NUM], eval_kwargs=kwargs)\n",
    "compiled_prompt_opt.save(\"pe_signature_optimizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚äπ Metrics definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(gold, pred, trace = None ):\n",
    "    actual_answer , pred_answer = gold.answer , pred.answer\n",
    "    print('actual_answer', actual_answer) \n",
    "    print('predicted_answer', pred_answer)\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(gold,pred)\n",
    "    return answer_EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíâ BootstrapFewShot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [00:01<01:09,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [00:23<11:04, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 3/50 [00:45<13:35, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 4/50 [01:27<20:48, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 5/50 [01:49<19:00, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 6/50 [02:31<22:44, 31.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 7/50 [03:14<24:55, 34.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 8/50 [03:36<21:31, 30.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 9/50 [04:24<24:42, 36.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [05:06<25:23, 38.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 11/50 [05:28<19:24, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 0\n",
      "Bootstrapped 4 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "teleprompter = BootstrapFewShot(metric=metric)\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(Router_module(), trainset=total_data[:DEV_NUM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_answer = ChainOfThought(Signature(question -> answer\n",
       "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Question to be routed to route', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'route option should be integer only no additional text', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
       "))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: Question to be routed to route\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "Answer: route option should be integer only no additional text\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the purpose of the study Ilomedin Treatment for Patients Having Undergone Primary Percutaneous Coronary Intervention (PCI) ?\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the need for constructing a novel TMSD based electrochemical detection strategy according to the research paper?\n",
      "Answer: 1\n",
      "\n",
      "---\n",
      "\n",
      "Question: How do mineral composition, surface topography, and surface charge influence DNA adsorption behavior and preservation on mineral substrates?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: Who is not recommending this work to be cited as a reference for projects?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: Where did the study Safety and Tolerability of RNS60 Given by IV to Healthy Subjects take place\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: In what ways does DigitISA offer an improvement over conventional immuno-sensing techniques?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: How do the results of utilizing GPLA jointly with biophysical modeling contribute to understanding the spatio-temporal dynamics observed in Utah array recordings?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: What does the second wave of muscle activity, time-locked to voluntary reach onset, indicate?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: What additional information beyond concentration measurements can DigitISA provide?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: Where did the study ROsuvastatin Pretreatment to Reduce MyocArdial Periprocedural Necrosis:Comparison With Atorvastatin Reloading (ROMAIIReload) take place\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: What type of health outcomes were measured in relation to medication adherence among the persons with MS in this study?\n",
      "Answer: 1\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the type of following study: Recipient Vessels for Free Tissue Flaps in Advanced Oncologic Defects of the Midface and Scalp\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: What treatment was given to the participants in Recipient Vessels for Free Tissue Flaps in Advanced Oncologic Defects of the Midface and Scalp\n",
      "Answer: 0\n",
      "\n",
      "---\n",
      "\n",
      "Question: What was the mean MARS-5 score among persons with MS in the study?\n",
      "Answer: 1\n",
      "\n",
      "---\n",
      "\n",
      "Question: What are the surgical treatments mentioned for hydrocephalus in the research paper?\n",
      "Answer: 1\n",
      "\n",
      "---\n",
      "\n",
      "Question: What was the reason for withdrawing the manuscript?\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: Where did the study Homeopathic Preparation Plumbum Metallicum for Lead Poisoning take place\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We need to identify the location where the study was conducted.\n",
      "Answer: 0\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag.save(\"router_bootstrapped_12_eg.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™ß BayesianSignature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [00:01<01:11,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer The specific individual or organization should be named.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [00:02<01:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 3/50 [00:24<08:15, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 4/50 [00:45<11:25, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 5/50 [01:07<13:03, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 6/50 [01:29<13:46, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer Ilomedin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 7/50 [01:50<14:06, 19.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 8/50 [02:12<14:10, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 9/50 [02:55<18:41, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [03:16<17:06, 25.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 11/50 [03:38<15:49, 24.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer C9H8O4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 12/50 [03:59<14:53, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 13/50 [04:22<14:13, 23.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 14/50 [04:44<13:41, 22.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 15/50 [05:05<13:01, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer Germany\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [05:28<12:44, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [05:50<12:17, 22.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer route option should be integer only no additional text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [06:32<15:09, 28.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer C17H18FN3O3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [06:54<11:16, 21.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 3\n",
      "Bootstrapped 1 full traces after 20 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [00:22<18:03, 22.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [00:44<17:37, 22.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 3/50 [01:05<17:05, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 0\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 4/50 [01:06<12:48, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 1\n",
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m teleprompter_baysian \u001b[38;5;241m=\u001b[39m BayesianSignatureOptimizer(prompt_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoute the respective question to the required service\u001b[39m\u001b[38;5;124m\"\u001b[39m, task_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroute integer option\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric\u001b[38;5;241m=\u001b[39mmetric, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_threads\u001b[38;5;241m=\u001b[39mNUM_THREADS, display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, display_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m compiled_router_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mteleprompter_baysian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRouter_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdevset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mDEV_NUM\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptuna_trials_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bootstrapped_demos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_labeled_demos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/teleprompt/signature_opt_bayesian.py:301\u001b[0m, in \u001b[0;36mBayesianSignatureOptimizer.compile\u001b[0;34m(self, student, devset, optuna_trials_num, max_bootstrapped_demos, max_labeled_demos, eval_kwargs, seed, view_data, view_examples)\u001b[0m\n\u001b[1;32m    298\u001b[0m             demo_candidates[\u001b[38;5;28mid\u001b[39m(module_p)]\u001b[38;5;241m.\u001b[39mappend(candidate_p\u001b[38;5;241m.\u001b[39mdemos)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Generate N candidate prompts\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m instruction_candidates, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_first_N_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Initialize variables to store the best program and its score\u001b[39;00m\n\u001b[1;32m    304\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/teleprompt/signature_opt_bayesian.py:176\u001b[0m, in \u001b[0;36mBayesianSignatureOptimizer._generate_first_N_candidates\u001b[0;34m(self, module, N, view_data, view_examples, demo_candidates, devset)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_model):\n\u001b[0;32m--> 176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observe_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservations:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m view_examples:\n\u001b[1;32m    179\u001b[0m     example_sets \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/teleprompt/signature_opt_bayesian.py:131\u001b[0m, in \u001b[0;36mBayesianSignatureOptimizer._observe_data\u001b[0;34m(self, trainset)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_observe_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainset):\n\u001b[1;32m    130\u001b[0m     upper_lim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trainset), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mview_data_batch_size)\n\u001b[0;32m--> 131\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDatasetDescriptor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mupper_lim\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     observations \u001b[38;5;241m=\u001b[39m observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    134\u001b[0m     skips \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/predict/predict.py:49\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dspy/predict/predict.py:90\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m template \u001b[38;5;241m=\u001b[39m signature_to_template(signature)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Note: query_only=True means the instructions and examples are not included.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# I'm not really sure why we'd want to do that, but it's there.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/dsp/primitives/predict.py:77\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[0;32m---> 77\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# BayesianSignature optimizer is not working \n",
    "# teleprompter_baysian = BayesianSignatureOptimizer(prompt_model=\"Route the respective question to the required service\", task_model=\"route integer option\", metric=metric, n=3)\n",
    "# kwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)\n",
    "# compiled_router_prompt = teleprompter_baysian.compile(Router_module(),  devset=total_data[:DEV_NUM], optuna_trials_num=100, max_bootstrapped_demos=1, max_labeled_demos=2, eval_kwargs=kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dspy.teleprompt.signature_opt_typed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mteleprompt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignature_opt_typed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize_signature\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m answer_exact_match\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypedChainOfThought\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dspy.teleprompt.signature_opt_typed'"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt.signature_opt_typed import optimize_signature\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from dspy.functional import TypedChainOfThought\n",
    "\n",
    "compiled_program = optimize_signature(\n",
    "    student=TypedChainOfThought(\"question -> answer\"),\n",
    "    evaluator=Evaluate(devset=total_data[:DEV_NUM], metric=answer_exact_match, num_threads=10, display_progress=True),\n",
    "    n_iterations=50,\n",
    ").program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ•Ô∏è Bootstrap few shot with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 2 traces per predictor.\n",
      "Will attempt to train 8 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 7/50 [00:00<00:00, 334.19it/s]\n",
      "[I 2024-03-12 19:53:33,909] A new study created in memory with name: no-name-e0f37a3a-69ec-448e-bba4-73dbd777928f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n",
      "actual_answer 1\n",
      "predicted_answer 2\n",
      "actual_answer 2\n",
      "predicted_answer 2\n",
      "actual_answer 3\n",
      "predicted_answer 2\n",
      "actual_answer 1\n",
      "predicted_answer 2\n",
      "actual_answer 3\n",
      "predicted_answer 2\n",
      "actual_answer 2\n",
      "predicted_answer 2\n",
      "Bootstrapped 2 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [00:01<00:04,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 2  (50.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:01<00:01,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 3  (33.3):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:21<00:19,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 2\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 4  (50.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:42<00:13, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:03<00:00, 12.76s/it]\n",
      "/opt/homebrew/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:142: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n",
      "[I 2024-03-12 19:54:37,873] Trial 0 finished with value: 60.0 and parameters: {'demo_index_for_generate_answer': 6}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 1\n",
      "Average Metric: 3 / 5  (60.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [00:21<01:26, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 2  (50.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:42<01:03, 21.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 3  (33.3):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:02<00:41, 20.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 4  (25.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:23<00:20, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:44<00:00, 20.93s/it]\n",
      "[I 2024-03-12 19:56:23,242] Trial 1 finished with value: 20.0 and parameters: {'demo_index_for_generate_answer': 0}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Average Metric: 1 / 5  (20.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [00:41<02:47, 41.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 0\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 2  (0.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [01:23<02:04, 41.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer patient outcome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 3  (0.0):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:43<01:03, 31.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "actual_answer 2\n",
      "predicted_answer 0\n",
      "Backing off 2.4 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.2 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.4 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 5.4 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 15.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 3.0 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.1 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 38.6 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 51.7 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 124.6 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 70.7 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 220.6 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 180.4 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 40.9 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 119.2 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 4  (0.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [11:35<04:13, 253.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Backing off 230.1 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [17:08<00:00, 205.77s/it]\n",
      "[I 2024-03-12 20:13:32,172] Trial 2 finished with value: 0.0 and parameters: {'demo_index_for_generate_answer': 5}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Average Metric: 0.0 / 5  (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.7 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [01:46<07:04, 106.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.4 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "actual_answer 3\n",
      "predicted_answer 0\n",
      "Backing off 3.1 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.9 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.5 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.7 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.5 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 5.0 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 10.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 8.5 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.9 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 18.0 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 8.4 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 57.5 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.0 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 63.8 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 32.6 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.0 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 202.8 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 54.0 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 8.1 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 48.8 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 135.5 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 220.4 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 184.5 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 2  (0.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [08:55<14:48, 296.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 2\n",
      "Backing off 136.6 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 232.3 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 355.2 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 269.7 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 121.6 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 3  (33.3):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [16:20<12:08, 364.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.0 / 4  (25.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [16:53<03:53, 233.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.0 / 5  (20.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [17:24<00:00, 208.99s/it]\n",
      "[I 2024-03-12 20:30:57,151] Trial 3 finished with value: 20.0 and parameters: {'demo_index_for_generate_answer': 12}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Average Metric: 1.0 / 5  (20.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.0 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.2 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.5 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.8 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.9 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.8 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.2 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.0 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.8 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 8.6 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 11.7 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 10.0 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 31.8 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 20.8 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 16.6 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.9 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 21.6 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 37.7 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.7 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.1 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.5 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 29.3 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 115.9 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 110.0 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.1 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 30.5 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 54.8 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 239.5 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 146.2 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 413.4 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 48.7 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 110.3 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 373.3 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 444.4 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 206.1 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 127.3 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [13:00<52:01, 780.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Backing off 177.8 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 126.3 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 97.4 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 3  (0.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [17:05<23:16, 465.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [17:05<02:53, 173.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [17:43<00:00, 212.63s/it]\n",
      "[I 2024-03-12 20:48:40,365] Trial 4 finished with value: 0.0 and parameters: {'demo_index_for_generate_answer': 3}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Average Metric: 0.0 / 5  (0.0%)\n",
      "actual_answer 2\n",
      "predicted_answer patient outcome\n",
      "actual_answer 3\n",
      "predicted_answer 0\n",
      "actual_answer 2\n",
      "predicted_answer 0\n",
      "actual_answer 3\n",
      "predicted_answer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 4  (0.0):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:00<00:00, 812.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.9 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.5 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 15.8 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:34<00:00, 30.90s/it]\n",
      "[I 2024-03-12 20:51:14,876] Trial 5 finished with value: 20.0 and parameters: {'demo_index_for_generate_answer': 5}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 1\n",
      "predicted_answer 1\n",
      "Average Metric: 1 / 5  (20.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.2 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.0 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.4 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.6 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.8 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 5.4 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.9 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 14.2 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.9 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.7 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 29.9 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.5 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.6 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 12.7 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 21.5 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 19.2 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 48.1 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.6 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 29.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 63.9 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 127.5 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 9.4 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 33.9 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 238.9 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 112.8 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 47.3 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 40.4 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):  20%|‚ñà‚ñà        | 1/5 [07:23<29:33, 443.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Backing off 180.1 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 505.5 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 167.4 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 153.3 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 68.8 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 54.8 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 274.6 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 241.3 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 208.5 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 2  (0.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [16:50<25:48, 516.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 2\n",
      "predicted_answer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 3  (0.0):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [16:59<09:29, 284.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [17:24<03:02, 182.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [17:42<00:00, 212.60s/it]\n",
      "[I 2024-03-12 21:08:57,897] Trial 6 finished with value: 0.0 and parameters: {'demo_index_for_generate_answer': 10}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Average Metric: 0.0 / 5  (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.5 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.6 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.3 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 1.0 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.5 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.4 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.9 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 7.2 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 3.4 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 4.0 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 12.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 28.3 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 6.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 2.5 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 29.8 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 27.7 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 31.2 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 56.4 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.8 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 21.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 51.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 55.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 103.8 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 17.1 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 13.8 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 109.9 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 28.5 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 104.5 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 117.8 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 203.6 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 58.3 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 84.0 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 142.9 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 328.4 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 102.3 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 274.5 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 91.4 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 352.5 seconds after 10 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 2.4 seconds after 3 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 3.1 seconds after 4 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 259.5 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 11.3 seconds after 5 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 29.6 seconds after 6 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 147.9 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 23.3 seconds after 7 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 71.7 seconds after 11 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n",
      "Backing off 42.3 seconds after 8 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 184.3 seconds after 9 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 37.1 seconds after 12 tries calling function <function GPT3.request at 0x13debb7e0> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 2  (0.0):  20%|‚ñà‚ñà        | 1/5 [17:23<1:09:35, 1043.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [17:43<03:01, 181.70s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-O0hnOySDX6ccyh9Epmb8cENl on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [19:29<00:00, 233.92s/it]\n",
      "[I 2024-03-12 21:28:27,540] Trial 7 finished with value: 0.0 and parameters: {'demo_index_for_generate_answer': 1}. Best is trial 0 with value: 60.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_answer 3\n",
      "predicted_answer 1\n",
      "Average Metric: 0.0 / 5  (0.0%)\n",
      "Best score: 60.0\n",
      "Best program: generate_answer = ChainOfThought(Signature(question -> answer\n",
      "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Question to be routed to route', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'route option should be integer only no additional text', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithOptuna\n",
    "\n",
    "fewshot_optuna_optimizer = BootstrapFewShotWithOptuna(metric=metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\n",
    "\n",
    "your_dspy_program_compiled = fewshot_optuna_optimizer.compile(student=Router_module(),max_demos = 2,  trainset=total_data[:DEV_NUM], valset=total_data[DEV_NUM: DEV_NUM + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_dspy_program_compiled.save(\"router_bootstrapped_few_shot_optuna.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: Question to be routed to route\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "Answer: route option should be integer only no additional text\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the role of CRISPR-Cas systems in Enterococcus faecalis strains?\n",
      "Reasoning: Let's think step by step in order to Answer: 2\n",
      "Answer: 2\n",
      "\n",
      "---\n",
      "\n",
      "Question: How many aromatic rings are present in the structure of Lisinopril?\n",
      "Reasoning: Let's think step by step in order to Answer: 1\n",
      "Answer:\u001b[32m 1\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
